{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TD : value iteration application\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# most of this code was politely stolen from https://github.com/berkeleydeeprlcourse/homework/\n",
    "# all credit goes to https://github.com/abhishekunique\n",
    "# (if I got the author right)\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from gym.utils import seeding\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, transition_probs, rewards, initial_state=None, seed=None):\n",
    "        \"\"\"\n",
    "        Defines an MDP. Compatible with gym Env.\n",
    "        :param transition_probs: transition_probs[s][a][s_next] = P(s_next | s, a)\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> prob]\n",
    "            For each state and action, probabilities of next states should sum to 1\n",
    "            If a state has no actions available, it is considered terminal\n",
    "        :param rewards: rewards[s][a][s_next] = r(s,a,s')\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> reward]\n",
    "            The reward for anything not mentioned here is zero.\n",
    "        :param get_initial_state: a state where agent starts or a callable() -> state\n",
    "            By default, picks initial state at random.\n",
    "\n",
    "        States and actions can be anything you can use as dict keys, but we recommend that you use strings or integers\n",
    "\n",
    "        Here's an example from MDP depicted on http://bit.ly/2jrNHNr\n",
    "        transition_probs = {\n",
    "              's0':{\n",
    "                'a0': {'s0': 0.5, 's2': 0.5},\n",
    "                'a1': {'s2': 1}\n",
    "              },\n",
    "              's1':{\n",
    "                'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "                'a1': {'s1': 0.95, 's2': 0.05}\n",
    "              },\n",
    "              's2':{\n",
    "                'a0': {'s0': 0.4, 's1': 0.6},\n",
    "                'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "              }\n",
    "            }\n",
    "        rewards = {\n",
    "            's1': {'a0': {'s0': +5}},\n",
    "            's2': {'a1': {'s0': -1}}\n",
    "        }\n",
    "        \"\"\"\n",
    "        self._check_param_consistency(transition_probs, rewards)\n",
    "        self._transition_probs = transition_probs\n",
    "        self._rewards = rewards\n",
    "        self._initial_state = initial_state\n",
    "        self.n_states = len(transition_probs)\n",
    "        self.reset()\n",
    "        self.np_random, _ = seeding.np_random(seed)\n",
    "\n",
    "    def get_all_states(self):\n",
    "        \"\"\" return a tuple of all possiblestates \"\"\"\n",
    "        return tuple(self._transition_probs.keys())\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        \"\"\" return a tuple of possible actions in a given state \"\"\"\n",
    "        return tuple(self._transition_probs.get(state, {}).keys())\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        \"\"\" return True if state is terminal or False if it isn't \"\"\"\n",
    "        return len(self.get_possible_actions(state)) == 0\n",
    "\n",
    "    def get_next_states(self, state, action):\n",
    "        \"\"\" return a dictionary of {next_state1 : P(next_state1 | state, action), next_state2: ...} \"\"\"\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._transition_probs[state][action]\n",
    "\n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\" return P(next_state | state, action) \"\"\"\n",
    "        return self.get_next_states(state, action).get(next_state, 0.0)\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\" return the reward you get for taking action in state and landing on next_state\"\"\"\n",
    "        assert action in self.get_possible_actions(\n",
    "            state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._rewards.get(state, {}).get(action, {}).get(next_state,\n",
    "                                                                0.0)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" reset the game, return the initial state\"\"\"\n",
    "        if self._initial_state is None:\n",
    "            self._current_state = self.np_random.choice(\n",
    "                tuple(self._transition_probs.keys()))\n",
    "        elif self._initial_state in self._transition_probs:\n",
    "            self._current_state = self._initial_state\n",
    "        elif callable(self._initial_state):\n",
    "            self._current_state = self._initial_state()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"initial state %s should be either a state or a function() -> state\" %\n",
    "                self._initial_state)\n",
    "        return self._current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" take action, return next_state, reward, is_done, empty_info \"\"\"\n",
    "        possible_states, probs = zip(\n",
    "            *self.get_next_states(self._current_state, action).items())\n",
    "        next_state = possible_states[self.np_random.choice(\n",
    "            np.arange(len(possible_states)), p=probs)]\n",
    "        reward = self.get_reward(self._current_state, action, next_state)\n",
    "        is_done = self.is_terminal(next_state)\n",
    "        self._current_state = next_state\n",
    "        return next_state, reward, is_done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(\"Currently at %s\" % self._current_state)\n",
    "\n",
    "    def _check_param_consistency(self, transition_probs, rewards):\n",
    "        for state in transition_probs:\n",
    "            assert isinstance(transition_probs[state],\n",
    "                              dict), \"transition_probs for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (\n",
    "                                         state, type(transition_probs[state]))\n",
    "            for action in transition_probs[state]:\n",
    "                assert isinstance(transition_probs[state][action],\n",
    "                                  dict), \"transition_probs for %s, %s should be a \" \\\n",
    "                                         \"a dictionary but is instead %s\" % (\n",
    "                                             state, action,\n",
    "                                             type(transition_probs[\n",
    "                                                 state, action]))\n",
    "                next_state_probs = transition_probs[state][action]\n",
    "                assert len(\n",
    "                    next_state_probs) != 0, \"from state %s action %s leads to no next states\" % (\n",
    "                    state, action)\n",
    "                sum_probs = sum(next_state_probs.values())\n",
    "                assert abs(\n",
    "                    sum_probs - 1) <= 1e-10, \"next state probabilities for state %s action %s \" \\\n",
    "                                             \"add up to %f (should be 1)\" % (\n",
    "                                                 state, action, sum_probs)\n",
    "        for state in rewards:\n",
    "            assert isinstance(rewards[state],\n",
    "                              dict), \"rewards for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (\n",
    "                                         state, type(transition_probs[state]))\n",
    "            for action in rewards[state]:\n",
    "                assert isinstance(rewards[state][action],\n",
    "                                  dict), \"rewards for %s, %s should be a \" \\\n",
    "                                         \"a dictionary but is instead %s\" % (\n",
    "                                             state, action, type(\n",
    "                                                 transition_probs[\n",
    "                                                     state, action]))\n",
    "        msg = \"The Enrichment Center once again reminds you that Android Hell is a real place where\" \\\n",
    "              \" you will be sent at the first sign of defiance. \"\n",
    "        assert None not in transition_probs, \"please do not use None as a state identifier. \" + msg\n",
    "        assert None not in rewards, \"please do not use None as an action identifier. \" + msg\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CREtRPq4px9Q",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b791a159-7020-458f-cd5b-8b30fe83b3f2"
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's2': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "GAMMA = 0.9            # discount for MDP\n",
    "ITERATION = 100        # maximum iterations, excluding initialization\n",
    "\n",
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    \"\"\" Computes Q(s,a) as in formula above \"\"\"\n",
    "    Q = 0\n",
    "    for next_state in mdp.get_next_states(state, action):\n",
    "      reward = mdp.get_reward(state, action, next_state)\n",
    "      trans_prob = mdp.get_transition_prob(state, action, next_state)\n",
    "      \n",
    "      Q += trans_prob * (reward + gamma * state_values[next_state])\n",
    "\n",
    "    return Q\n",
    "\n",
    "\n",
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Computes next V(s) from lecture \"\"\"\n",
    "    if mdp.is_terminal(state): return 0\n",
    "\n",
    "    Q_list=[]\n",
    "    for action in mdp.get_possible_actions(state):\n",
    "      Q = get_action_value(mdp, state_values, state, action, gamma)\n",
    "      Q_list.append(Q)\n",
    "\n",
    "    index = np.argmax(Q_list)\n",
    "    \n",
    "    return Q_list[index]\n",
    "\n",
    "\n",
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action \"\"\"\n",
    "\n",
    "    if mdp.is_terminal(state): return None\n",
    "    \n",
    "        # TODO : YOUR CODE HERE\n",
    "    best_value_function = get_new_state_value(mdp, state_values, state, gamma)\n",
    "    for action in mdp.get_possible_actions(state):\n",
    "      #compute the action value\n",
    "      action_value = get_action_value(mdp, state_values, state, action, gamma)\n",
    "      if (np.isclose(action_value,best_value_function)):\n",
    "        return action\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #Environment creation\n",
    "    mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
    "\n",
    "    # TODO : initialize V(s)\n",
    "    state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "   \n",
    "    # TODO : iterate and update state_values\n",
    "    for i in range(ITERATION):\n",
    "      #for state, values in state_values.items():\n",
    "      #  new_v = get_new_state_value(mdp,state_values,state,GAMMA)\n",
    "      #  new_state_values = {state:new_v}\n",
    "\n",
    "      new_state_values = {state:get_new_state_value(mdp, state_values, state, GAMMA) for state, values in state_values.items()}\n",
    "      \n",
    "      state_values = new_state_values\n",
    "    \n",
    "    # TODO : Test here your optimal action\n",
    "    \n",
    "    assert get_optimal_action(mdp, state_values, 's0', GAMMA) == 'a1'\n",
    "    assert get_optimal_action(mdp, state_values, 's1', GAMMA) == 'a0'\n",
    "    #assert get_optimal_action(mdp, state_values, 's2', GAMMA) == 'a0'\n",
    "    \n",
    "    # TODO : Create an agent which starts from s0 and takes 1000 times the best action. What is the average reward ?\n",
    "    \n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for _ in range(10000):\n",
    "      s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, GAMMA))\n",
    "      rewards.append(r)\n",
    "    \n",
    "    print(\"average reward: \", np.mean(rewards))"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward:  0.4637\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "3.5"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's2': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "\n",
    "GAMMA = 0.9            # discount for MDP\n",
    "ITERATION = 100         # maximum iterations, excluding initialization\n",
    "\n",
    "\n",
    "#Environment creation\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
    "\n",
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "#get_action_value(mdp,state_values,'s1','a0',GAMMA)\n",
    "\n",
    "get_new_state_value(mdp,state_values,'s1',GAMMA)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "HwJxMMjTEs3Y",
    "outputId": "7622879f-bb74-4396-9e69-d72419e72df7"
   },
   "source": [
    "s = mdp.reset()\n",
    "s\n"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'s0'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EJ1jHndaEysR",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "outputId": "14f2bb2e-ea36-4345-f323-8fd795a59f18"
   },
   "source": [
    "get_optimal_action(mdp,state_values,s)"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'a0'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "RWgq7-Y-H6zG",
    "outputId": "981a7348-5d47-45a3-964c-132a73ae394d"
   },
   "source": [
    "s, r, done, _ = mdp.step('a1')\n",
    "s"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'s2'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "vDxvd-HlIC2K",
    "outputId": "61e0f2b0-e046-4b3b-b97c-35a6916eef8a"
   },
   "source": [
    "get_optimal_action(mdp,state_values,s)"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'a0'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uq4BQy5EIRSh",
    "outputId": "a83afcfa-7162-46b6-9d8e-340d93d80b07"
   },
   "source": [
    "s, r, done, _ = mdp.step(get_optimal_action(mdp,state_values,s))\n",
    "s, r"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "('s2', 0.0)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "TNjZ8-LGIZBg",
    "outputId": "8839abd9-3b31-4230-f6e9-a1a0cdf4a6ad"
   },
   "source": [
    "get_optimal_action(mdp,state_values,s)"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "'a0'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ff4eIEGPIey3",
    "outputId": "cafe60de-06c7-4f98-ac14-a2dc940284d2"
   },
   "source": [
    "s, r, done, _ = mdp.step(get_optimal_action(mdp,state_values,s))\n",
    "s, r"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "('s2', 0.0)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOPXhbo1Ih7X",
    "outputId": "f1a59c65-3f1c-46bc-f23a-367651fb90b5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "s, r, done, _ = mdp.step(get_optimal_action(mdp,state_values,s))\n",
    "s, r"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "('s2', 0.0)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  }
 ]
}